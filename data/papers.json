[
  {
    "title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models",
    "authors": "Yi Jing, Zijun Yao, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, Juanzi Li",
    "summary": "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
    "published": "2025-02-27T18:16:47Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20344v1"
  },
  {
    "title": "Transfer Learning in Latent Contextual Bandits with Covariate Shift\n  Through Causal Transportability",
    "authors": "Mingwei Deng, Ville Kyrki, Dominik Baumann",
    "summary": "Transferring knowledge from one environment to another is an essential\nability of intelligent systems. Nevertheless, when two environments are\ndifferent, naively transferring all knowledge may deteriorate the performance,\na phenomenon known as negative transfer. In this paper, we address this issue\nwithin the framework of multi-armed bandits from the perspective of causal\ninference. Specifically, we consider transfer learning in latent contextual\nbandits, where the actual context is hidden, but a potentially high-dimensional\nproxy is observable. We further consider a covariate shift in the context\nacross environments. We show that naively transferring all knowledge for\nclassical bandit algorithms in this setting led to negative transfer. We then\nleverage transportability theory from causal inference to develop algorithms\nthat explicitly transfer effective knowledge for estimating the causal effects\nof interest in the target environment. Besides, we utilize variational\nautoencoders to approximate causal effects under the presence of a\nhigh-dimensional proxy. We test our algorithms on synthetic and semi-synthetic\ndatasets, empirically demonstrating consistently improved learning efficiency\nacross different proxies compared to baseline algorithms, showing the\neffectiveness of our causal framework in transferring knowledge.",
    "published": "2025-02-27T14:52:23Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20153v1"
  },
  {
    "title": "Identifiable Multi-View Causal Discovery Without Non-Gaussianity",
    "authors": "Ambroise Heurtebise, Omar Chehab, Pierre Ablin, Alexandre Gramfort, Aapo Hyv\u00e4rinen",
    "summary": "We propose a novel approach to linear causal discovery in the framework of\nmulti-view Structural Equation Models (SEM). Our proposed model relaxes the\nwell-known assumption of non-Gaussian disturbances by alternatively assuming\ndiversity of variances over views, making it more broadly applicable. We prove\nthe identifiability of all the parameters of the model without any further\nassumptions on the structure of the SEM other than it being acyclic. We further\npropose an estimation algorithm based on recent advances in multi-view\nIndependent Component Analysis (ICA). The proposed methodology is validated\nthrough simulations and application on real neuroimaging data, where it enables\nthe estimation of causal graphs between brain regions.",
    "published": "2025-02-27T14:06:14Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20115v1"
  },
  {
    "title": "Sanity Checking Causal Representation Learning on a Simple Real-World\n  System",
    "authors": "Juan L. Gamella, Simon Bing, Jakob Runge",
    "summary": "We evaluate methods for causal representation learning (CRL) on a simple,\nreal-world system where these methods are expected to work. The system consists\nof a controlled optical experiment specifically built for this purpose, which\nsatisfies the core assumptions of CRL and where the underlying causal factors\n(the inputs to the experiment) are known, providing a ground truth. We select\nmethods representative of different approaches to CRL and find that they all\nfail to recover the underlying causal factors. To understand the failure modes\nof the evaluated algorithms, we perform an ablation on the data by substituting\nthe real data-generating process with a simpler synthetic equivalent. The\nresults reveal a reproducibility problem, as most methods already fail on this\nsynthetic ablation despite its simple data-generating process. Additionally, we\nobserve that common assumptions on the mixing function are crucial for the\nperformance of some of the methods but do not hold in the real data. Our\nefforts highlight the contrast between the theoretical promise of the state of\nthe art and the challenges in its application. We hope the benchmark serves as\na simple, real-world sanity check to further develop and validate methodology,\nbridging the gap towards CRL methods that work in practice. We make all code\nand datasets publicly available at github.com/simonbing/CRLSanityCheck",
    "published": "2025-02-27T13:56:54Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20099v1"
  },
  {
    "title": "A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera\n  Relocalization",
    "authors": "Yejun Zhang, Shuzhe Wang, Juho Kannala",
    "summary": "Visual localization involves estimating the 6-degree-of-freedom (6-DoF)\ncamera pose within a known scene. A critical step in this process is\nidentifying pixel-to-point correspondences between 2D query images and 3D\nmodels. Most advanced approaches currently rely on extensive visual descriptors\nto establish these correspondences, facing challenges in storage, privacy\nissues and model maintenance. Direct 2D-3D keypoint matching without visual\ndescriptors is becoming popular as it can overcome those challenges. However,\nexisting descriptor-free methods suffer from low accuracy or heavy computation.\nAddressing this gap, this paper introduces the Angle-Annular Graph Neural\nNetwork (A2-GNN), a simple approach that efficiently learns robust geometric\nstructural representations with annular feature extraction. Specifically, this\napproach clusters neighbors and embeds each group's distance information and\nangle as supplementary information to capture local structures. Evaluation on\nmatching and visual localization datasets demonstrates that our approach\nachieves state-of-the-art accuracy with low computational overhead among visual\ndescription-free methods. Our code will be released on\nhttps://github.com/YejunZhang/a2-gnn.",
    "published": "2025-02-27T12:25:30Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20036v1"
  },
  {
    "title": "Offline Reinforcement Learning via Inverse Optimization",
    "authors": "Ioannis Dimanidis, Tolga Ok, Peyman Mohajerin Esfahani",
    "summary": "Inspired by the recent successes of Inverse Optimization (IO) across various\napplication domains, we propose a novel offline Reinforcement Learning (ORL)\nalgorithm for continuous state and action spaces, leveraging the convex loss\nfunction called ``sub-optimality loss\" from the IO literature. To mitigate the\ndistribution shift commonly observed in ORL problems, we further employ a\nrobust and non-causal Model Predictive Control (MPC) expert steering a nominal\nmodel of the dynamics using in-hindsight information stemming from the model\nmismatch. Unlike the existing literature, our robust MPC expert enjoys an exact\nand tractable convex reformulation. In the second part of this study, we show\nthat the IO hypothesis class, trained by the proposed convex loss function,\nenjoys ample expressiveness and achieves competitive performance comparing with\nthe state-of-the-art (SOTA) methods in the low-data regime of the MuJoCo\nbenchmark while utilizing three orders of magnitude fewer parameters, thereby\nrequiring significantly fewer computational resources. To facilitate the\nreproducibility of our results, we provide an open-source package implementing\nthe proposed algorithms and the experiments.",
    "published": "2025-02-27T12:11:44Z",
    "pdf_link": "http://arxiv.org/pdf/2502.20030v1"
  },
  {
    "title": "WaveGAS: Waveform Relaxation for Scaling Graph Neural Networks",
    "authors": "Jana Vatter, Mykhaylo Zayats, Marcos Mart\u00ednez Galindo, Vanessa L\u00f3pez, Ruben Mayer, Hans-Arno Jacobsen, Hoang Thanh Lam",
    "summary": "With the ever-growing size of real-world graphs, numerous techniques to\novercome resource limitations when training Graph Neural Networks (GNNs) have\nbeen developed. One such approach, GNNAutoScale (GAS), uses graph partitioning\nto enable training under constrained GPU memory. GAS also stores historical\nembedding vectors, which are retrieved from one-hop neighbors in other\npartitions, ensuring critical information is captured across partition\nboundaries. The historical embeddings which come from the previous training\niteration are stale compared to the GAS estimated embeddings, resulting in\napproximation errors of the training algorithm. Furthermore, these errors\naccumulate over multiple layers, leading to suboptimal node embeddings. To\naddress this shortcoming, we propose two enhancements: first, WaveGAS, inspired\nby waveform relaxation, performs multiple forward passes within GAS before the\nbackward pass, refining the approximation of historical embeddings and\ngradients to improve accuracy; second, a gradient-tracking method that stores\nand utilizes more accurate historical gradients during training. Empirical\nresults show that WaveGAS enhances GAS and achieves better accuracy, even\noutperforming methods that train on full graphs, thanks to its robust\nestimation of node embeddings.",
    "published": "2025-02-27T11:10:42Z",
    "pdf_link": "http://arxiv.org/pdf/2502.19986v1"
  },
  {
    "title": "Efficient and Universal Neural-Network Decoder for Stabilizer-Based\n  Quantum Error Correction",
    "authors": "Gengyuan Hu, Wanli Ouyang, Chao-Yang Lu, Chen Lin, Han-Sen Zhong",
    "summary": "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
    "published": "2025-02-27T10:56:53Z",
    "pdf_link": "http://arxiv.org/pdf/2502.19971v1"
  },
  {
    "title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
    "authors": "Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie",
    "summary": "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft.",
    "published": "2025-02-27T09:18:04Z",
    "pdf_link": "http://arxiv.org/pdf/2502.19902v1"
  },
  {
    "title": "Economic Causal Inference Based on DML Framework: Python Implementation\n  of Binary and Continuous Treatment Variables",
    "authors": "Shunxin Yao",
    "summary": "This study utilizes a simulated dataset to establish Python code for Double\nMachine Learning (DML) using Anaconda's Jupyter Notebook and the DML software\npackage from GitHub. The research focuses on causal inference experiments for\nboth binary and continuous treatment variables. The findings reveal that the\nDML model demonstrates relatively stable performance in calculating the Average\nTreatment Effect (ATE) and its robustness metrics. However, the study also\nhighlights that the computation of Conditional Average Treatment Effect (CATE)\nremains a significant challenge for future DML modeling, particularly in the\ncontext of continuous treatment variables. This underscores the need for\nfurther research and development in this area to enhance the model's\napplicability and accuracy.",
    "published": "2025-02-27T09:13:58Z",
    "pdf_link": "http://arxiv.org/pdf/2502.19898v1"
  }
]