[
  {
    "title": "Testing the limits of fine-tuning to improve reasoning in vision\n  language models",
    "authors": "Luca M. Schulze Buschoff, Konstantinos Voudouris, Elif Akata, Matthias Bethge, Joshua B. Tenenbaum, Eric Schulz",
    "summary": "Pre-trained vision language models still fall short of human visual\ncognition. In an effort to improve visual cognition and align models with human\nbehavior, we introduce visual stimuli and human judgments on visual cognition\ntasks, allowing us to systematically evaluate performance across cognitive\ndomains under a consistent environment. We fine-tune models on ground truth\ndata for intuitive physics and causal reasoning and find that this improves\nmodel performance in the respective fine-tuning domain. Furthermore, it can\nimprove model alignment with human behavior. However, we find that fine-tuning\ndoes not contribute to robust human-like generalization to data with other\nvisual characteristics or to tasks in other cognitive domains.",
    "published": "2025-02-21T18:58:30Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15678v1"
  },
  {
    "title": "Predicting gene essentiality and drug response from perturbation screens\n  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and\n  Predictors",
    "authors": "Barbara Bodinier, Gaetan Dissez, Linus Bleistein, Antonin Dauvin",
    "summary": "Preclinical perturbation screens, where the effects of genetic, chemical, or\nenvironmental perturbations are systematically tested on disease models, hold\nsignificant promise for machine learning-enhanced drug discovery due to their\nscale and causal nature. Predictive models can infer perturbation responses for\npreviously untested disease models based on molecular profiles. These in silico\nlabels can expand databases and guide experimental prioritization.\n  However, modelling perturbation-specific effects and generating robust\nprediction performances across diverse biological contexts remain elusive. We\nintroduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel\nensemble framework to improve robustness and generalization. LEAP leverages\nmultiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO\nregressors. By combining diverse gene expression representation models learned\nfrom different random initializations, LEAP consistently outperforms\nstate-of-the-art approaches in predicting gene essentiality or drug responses\nin unseen cell lines, tissues and disease models. Notably, our results show\nthat ensembling representation models, rather than prediction models alone,\nyields superior predictive performance.\n  Beyond its performance gains, LEAP is computationally efficient, requires\nminimal hyperparameter tuning and can therefore be readily incorporated into\ndrug discovery pipelines to prioritize promising targets and support\nbiomarker-driven stratification. The code and datasets used in this work are\nmade publicly available.",
    "published": "2025-02-21T18:12:36Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15646v1"
  },
  {
    "title": "Causal Modeling of fMRI Time-series for Interpretable Autism Spectrum\n  Disorder Classification",
    "authors": "Peiyu Duan, Nicha C. Dvornek, Jiyao Wang, Lawrence H. Staib, James S. Duncan",
    "summary": "Autism spectrum disorder (ASD) is a neurological and developmental disorder\nthat affects social and communicative behaviors. It emerges in early life and\nis generally associated with lifelong disabilities. Thus, accurate and early\ndiagnosis could facilitate treatment outcomes for those with ASD. Functional\nmagnetic resonance imaging (fMRI) is a useful tool that measures changes in\nbrain signaling to facilitate our understanding of ASD. Much effort is being\nmade to identify ASD biomarkers using various connectome-based machine learning\nand deep learning classifiers. However, correlation-based models cannot capture\nthe non-linear interactions between brain regions. To solve this problem, we\nintroduce a causality-inspired deep learning model that uses time-series\ninformation from fMRI and captures causality among ROIs useful for ASD\nclassification. The model is compared with other baseline and state-of-the-art\nmodels with 5-fold cross-validation on the ABIDE dataset. We filtered the\ndataset by choosing all the images with mean FD less than 15mm to ensure data\nquality. Our proposed model achieved the highest average classification\naccuracy of 71.9% and an average AUC of 75.8%. Moreover, the inter-ROI\ncausality interpretation of the model suggests that the left precuneus, right\nprecuneus, and cerebellum are placed in the top 10 ROIs in inter-ROI causality\namong the ASD population. In contrast, these ROIs are not ranked in the top 10\nin the control population. We have validated our findings with the literature\nand found that abnormalities in these ROIs are often associated with ASD.",
    "published": "2025-02-21T17:12:35Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15595v1"
  },
  {
    "title": "Simulating Noncausality with Quantum Control of Causal Orders",
    "authors": "Anna Steffinlongo, Hippolyte Dourdent",
    "summary": "Logical consistency with free local operations is compatible with non-trivial\nclassical communications, where all parties can be both in each other's past\nand future - a phenomenon known as noncausality. Noncausal processes, such as\nthe ''Lugano (AF/BW) process'', violate causal inequalities, yet their physical\nrealizability remains an open question. In contrast, the quantum switch - a\nphysically realizable process with indefinite causal order - can only generate\ncausal correlations. Building on a recently established equivalence [Kunjwal\nand Baumeler, PRL 131, 120201 (2023)] between the SHIFT measurement, which\nexhibits nonlocality without entanglement, and the Lugano process, we\ndemonstrate that the SHIFT measurement can be implemented using the quantum\nswitch in a scenario with quantum inputs. This shows that the structure of the\nLugano process can be simulated by a quantum switch and that successful SHIFT\ndiscrimination witnesses causal nonseparability rather than noncausality,\nrefuting prior claims. Finally, we identify a broad class of ''superposition of\nclassical communications'' derived from classical processes without global past\ncapable of realizing similar causally indefinite measurements. We examine these\nresults in relation to the ongoing debate on implementations of indefinite\ncausal orders.",
    "published": "2025-02-21T16:39:55Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15579v1"
  },
  {
    "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
    "authors": "Martina Miliani, Serenna Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci",
    "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
    "published": "2025-02-21T14:23:14Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15487v1"
  },
  {
    "title": "Game State and Spatio-temporal Action Detection in Soccer using Graph\n  Neural Networks and 3D Convolutional Networks",
    "authors": "Jeremie Ochin, Guillaume Devineau, Bogdan Stanciulescu, Sotiris Manitsaris",
    "summary": "Soccer analytics rely on two data sources: the player positions on the pitch\nand the sequences of events they perform. With around 2000 ball events per\ngame, their precise and exhaustive annotation based on a monocular video stream\nremains a tedious and costly manual task. While state-of-the-art\nspatio-temporal action detection methods show promise for automating this task,\nthey lack contextual understanding of the game. Assuming professional players'\nbehaviors are interdependent, we hypothesize that incorporating surrounding\nplayers' information such as positions, velocity and team membership can\nenhance purely visual predictions. We propose a spatio-temporal action\ndetection approach that combines visual and game state information via Graph\nNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating\nimproved metrics through game state integration.",
    "published": "2025-02-21T13:41:38Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15462v1"
  },
  {
    "title": "State-space kinetic Ising model reveals task-dependent entropy flow in\n  sparsely active nonequilibrium neuronal dynamics",
    "authors": "Ken Ishihara, Hideaki Shimazaki",
    "summary": "Neuronal ensemble activity, including coordinated and oscillatory patterns,\nexhibits hallmarks of nonequilibrium systems with time-asymmetric trajectories\nto maintain their organization. However, assessing time asymmetry from neuronal\nspiking activity remains challenging. The kinetic Ising model provides a\nframework for studying the causal, nonequilibrium dynamics in spiking recurrent\nneural networks. Recent theoretical advances in this model have enabled\ntime-asymmetry estimation from large-scale steady-state data. Yet, neuronal\nactivity often exhibits time-varying firing rates and coupling strengths,\nviolating steady-state assumption. To overcome these limitations, we developed\na state-space kinetic Ising model that accounts for non-stationary and\nnonequilibrium properties of neural systems. This approach incorporates a\nmean-field method for estimating time-varying entropy flow, a key measure for\nmaintaining the system's organization by dissipation. Applying this method to\nmouse visual cortex data revealed greater variability in causal couplings\nduring task engagement despite the reduced neuronal activity with increased\nsparsity. Moreover, higher-performing mice exhibited increased entropy flow in\nhigher-firing neurons during task engagement, suggesting that stronger directed\nactivity emerged in a fewer neurons within sparsely active populations. These\nfindings underscore the model's utility in uncovering intricate asymmetric\ncausal dynamics in neuronal ensembles and linking them to behavior through the\nthermodynamic underpinnings of neural computation.",
    "published": "2025-02-21T13:08:13Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15440v1"
  },
  {
    "title": "Modeling Compact Objects in $f(R)$ Gravity: Application of Buchdahl-I\n  Metric with Chaplygin Equation of State",
    "authors": "A. Zahra, S. A. Mardan, Muhammad Bilal Riaz, S. Saleem",
    "summary": "This paper investigates realistic anisotropic matter configurations for\nspherical symmetry in the framework of $f(R)$ gravity. The solutions obtained\nfrom Buchdahl-I metric are used to determine the behavior of PSR J0740+6620,\nPSR J0348+0432 and 4U 1608-52 with Starobinsky model. Analysis of physical\nparameters such as density, pressure, and anisotropy is illustrated through\ngraphs, and the stability of compact objects is investigated by energy and\ncausality conditions. We will also discuss the behavior of gravitational,\nhydrostatic and anisotropic forces, gravitational redshift and adiabatic index.\nAt the theoretical and astrophysical scales, the graphical representations\nvalidate the practical and realistic $f(R)$ gravity models.",
    "published": "2025-02-21T11:13:56Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15388v1"
  },
  {
    "title": "Drug-Target Interaction/Affinity Prediction: Deep Learning Models and\n  Advances Review",
    "authors": "Ali Vefghi, Zahed Rahmati, Mohammad Akbari",
    "summary": "Drug discovery remains a slow and expensive process that involves many steps,\nfrom detecting the target structure to obtaining approval from the Food and\nDrug Administration (FDA), and is often riddled with safety concerns. Accurate\nprediction of how drugs interact with their targets and the development of new\ndrugs by using better methods and technologies have immense potential to speed\nup this process, ultimately leading to faster delivery of life-saving\nmedications. Traditional methods used for drug-target interaction prediction\nshow limitations, particularly in capturing complex relationships between drugs\nand their targets. As an outcome, deep learning models have been presented to\novercome the challenges of interaction prediction through their precise and\nefficient end results. By outlining promising research avenues and models, each\nwith a different solution but similar to the problem, this paper aims to give\nresearchers a better idea of methods for even more accurate and efficient\nprediction of drug-target interaction, ultimately accelerating the development\nof more effective drugs. A total of 180 prediction methods for drug-target\ninteractions were analyzed throughout the period spanning 2016 to 2025 using\ndifferent frameworks based on machine learning, mainly deep learning and graph\nneural networks. Additionally, this paper discusses the novelty, architecture,\nand input representation of these models.",
    "published": "2025-02-21T10:00:43Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15346v1"
  },
  {
    "title": "Analyzing the Inner Workings of Transformers in Compositional\n  Generalization",
    "authors": "Ryoma Kumon, Hitomi Yanaka",
    "summary": "The compositional generalization abilities of neural models have been sought\nafter for human-like linguistic competence. The popular method to evaluate such\nabilities is to assess the models' input-output behavior. However, that does\nnot reveal the internal mechanisms, and the underlying competence of such\nmodels in compositional generalization remains unclear. To address this\nproblem, we explore the inner workings of a Transformer model by finding an\nexisting subnetwork that contributes to the generalization performance and by\nperforming causal analyses on how the model utilizes syntactic features. We\nfind that the model depends on syntactic features to output the correct answer,\nbut that the subnetwork with much better generalization performance than the\nwhole model relies on a non-compositional algorithm in addition to the\nsyntactic features. We also show that the subnetwork improves its\ngeneralization performance relatively slowly during the training compared to\nthe in-distribution one, and the non-compositional solution is acquired in the\nearly stages of the training.",
    "published": "2025-02-21T08:07:53Z",
    "pdf_link": "http://arxiv.org/pdf/2502.15277v1"
  }
]